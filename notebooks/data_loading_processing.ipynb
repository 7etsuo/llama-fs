{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = \"../sample_data/\"\n",
    "reader = SimpleDirectoryReader(input_dir=file_dir)\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2402.05602v1.pdf', 'd20e3b9a-9981-41c4-a6f7-dc1bafff4761.JPG']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='adb5bb59-c7af-4bbf-9d2d-788d008ed952', embedding=None, metadata={'page_label': '1', 'file_name': '2402.05602v1.pdf', 'file_path': '/Users/ashwin/side_projects/llama-fs/notebooks/../sample_data/2402.05602v1.pdf', 'file_type': 'application/pdf', 'file_size': 14982917, 'creation_date': '2024-05-11', 'last_modified_date': '2024-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='AttnLRP: Attention-Aware Layer-wise Relevance Propagation\\nfor Transformers\\nReduan Achtibat1Sayed Mohammad Vakilzadeh Hatefi1Maximilian Dreyer1\\nAakriti Jain1Thomas Wiegand1,2,3Sebastian Lapuschkin1,†Wojciech Samek1,2,3,†\\n1Fraunhofer Heinrich-Hertz-Institute, 10587 Berlin, Germany\\n2Technische Universit ¨at Berlin, 10587 Berlin, Germany\\n3BIFOLD – Berlin Institute for the Foundations of Learning and Data, 10587 Berlin, Germany\\n†corresponding authors: {wojciech.samek,sebastian.lapuschkin }@hhi.fraunhofer.de\\nAbstract\\nLarge Language Models are prone to biased\\npredictions and hallucinations, underlining the\\nparamount importance of understanding their\\nmodel-internal reasoning process. However,\\nachieving faithful attributions for the entirety\\nof a black-box transformer model and main-\\ntaining computational efficiency is an unsolved\\nchallenge. By extending the Layer-wise Rel-\\nevance Propagation attribution method to han-\\ndle attention layers, we address these challenges\\neffectively. While partial solutions exist, our\\nmethod is the first to faithfully and holistically\\nattribute not only input but also latent represen-\\ntations of transformer models with the computa-\\ntional efficiency similar to a singular backward\\npass. Through extensive evaluations against ex-\\nisting methods on Llama 2, Flan-T5 and the Vi-\\nsion Transformer architecture, we demonstrate\\nthat our proposed approach surpasses alterna-\\ntive methods in terms of faithfulness and enables\\nthe understanding of latent representations, open-\\ning up the door for concept-based explanations.\\nWe provide an open-source implementation on\\nGitHub1.\\n1. Introduction\\nThe attention mechanism (Vaswani et al., 2017) became\\nan essential component of large transformers due to its\\nunique ability to handle multimodality and to scale to bil-\\nlions of training samples. While these models demonstrate\\nimpressive performance in text and image generation, they\\nare prone to biased predictions and hallucinations (Huang\\net al., 2023), which hamper their widespread adoption.\\nTo overcome these limitations, it is crucial to understand\\n1https://github.com/rachtibat/LRP-for-Transformers\\nexplanation for “ dog”inputfaithfulness\\nlatent \\nattributionscomputational \\nefficiencyA ttnLRP (ours)SmoothGr adGr ad×A ttnRollA tMan\\nFigure 1. By optimizing LRP for transformer-based architectures,\\nour LRP variant outperforms other state-of-the-art methods in\\nterms of explanation faithfulness and computational efficiency.\\nWe further are able to explain latent neurons inside and outside\\nthe attention module, allowing us to interact with the model. A\\nmore detailed discussion on the differences between AttnLRP and\\nother LRP variants can be found in Appendix A.2.2. Heatmaps\\nfor other methods are illustrated in Appendix Figure 6. Legend:\\nhighly ( +), semi- ( ◦), not suited ( −). Credit: Nataba/iStock.\\nthe latent reasoning process of transformer models. Re-\\nsearchers started using the attention mechanism of trans-\\nformers as a means to understand how input tokens inter-\\nact with each other. Attention maps contain rich informa-\\ntion about the data distribution (Clark et al., 2019; Caron\\net al., 2021), even allowing for image data segmentation.\\nHowever, attention, by itself, is inadequate for compre-\\nhending the full spectrum of model behavior (Wiegreffe\\nand Pinter, 2019). Similar to latent activations, attention\\nis not class-specific and solely provides an explanation for\\nthe softmax output (in attention layers) while disregarding\\nother model components. Recent works (Geva et al., 2021;\\nDai et al., 2022), e.g., have discovered that factual knowl-\\nedge in Large Language Models (LLMs) is stored in Feed-\\nForward Network (FFN) neurons, separate from attention\\nlayers. Further, attention-based attribution methods such\\nas rollout (Abnar and Zuidema, 2020; Chefer et al., 2021a)\\n1arXiv:2402.05602v1  [cs.CL]  8 Feb 2024', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3_hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
