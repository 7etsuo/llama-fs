{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import ollama\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = \"../sample_data/\"\n",
    "reader = SimpleDirectoryReader(input_dir=file_dir)\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id_': '4ab69592-b864-4de6-a4d0-3fe6e558d950',\n",
       " 'embedding': None,\n",
       " 'metadata': {'page_label': '1',\n",
       "  'file_name': '2402.05602v1.pdf',\n",
       "  'file_path': '/Users/ashwin/side_projects/llama-fs/notebooks/../sample_data/2402.05602v1.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 14982917,\n",
       "  'creation_date': '2024-05-11',\n",
       "  'last_modified_date': '2024-05-08'},\n",
       " 'excluded_embed_metadata_keys': ['file_name',\n",
       "  'file_type',\n",
       "  'file_size',\n",
       "  'creation_date',\n",
       "  'last_modified_date',\n",
       "  'last_accessed_date'],\n",
       " 'excluded_llm_metadata_keys': ['file_name',\n",
       "  'file_type',\n",
       "  'file_size',\n",
       "  'creation_date',\n",
       "  'last_modified_date',\n",
       "  'last_accessed_date'],\n",
       " 'relationships': {},\n",
       " 'text': 'AttnLRP: Attention-Aware Layer-wise Relevance Propagation\\nfor Transformers\\nReduan Achtibat1Sayed Mohammad Vakilzadeh Hatefi1Maximilian Dreyer1\\nAakriti Jain1Thomas Wiegand1,2,3Sebastian Lapuschkin1,†Wojciech Samek1,2,3,†\\n1Fraunhofer Heinrich-Hertz-Institute, 10587 Berlin, Germany\\n2Technische Universit ¨at Berlin, 10587 Berlin, Germany\\n3BIFOLD – Berlin Institute for the Foundations of Learning and Data, 10587 Berlin, Germany\\n†corresponding authors: {wojciech.samek,sebastian.lapuschkin }@hhi.fraunhofer.de\\nAbstract\\nLarge Language Models are prone to biased\\npredictions and hallucinations, underlining the\\nparamount importance of understanding their\\nmodel-internal reasoning process. However,\\nachieving faithful attributions for the entirety\\nof a black-box transformer model and main-\\ntaining computational efficiency is an unsolved\\nchallenge. By extending the Layer-wise Rel-\\nevance Propagation attribution method to han-\\ndle attention layers, we address these challenges\\neffectively. While partial solutions exist, our\\nmethod is the first to faithfully and holistically\\nattribute not only input but also latent represen-\\ntations of transformer models with the computa-\\ntional efficiency similar to a singular backward\\npass. Through extensive evaluations against ex-\\nisting methods on Llama 2, Flan-T5 and the Vi-\\nsion Transformer architecture, we demonstrate\\nthat our proposed approach surpasses alterna-\\ntive methods in terms of faithfulness and enables\\nthe understanding of latent representations, open-\\ning up the door for concept-based explanations.\\nWe provide an open-source implementation on\\nGitHub1.\\n1. Introduction\\nThe attention mechanism (Vaswani et al., 2017) became\\nan essential component of large transformers due to its\\nunique ability to handle multimodality and to scale to bil-\\nlions of training samples. While these models demonstrate\\nimpressive performance in text and image generation, they\\nare prone to biased predictions and hallucinations (Huang\\net al., 2023), which hamper their widespread adoption.\\nTo overcome these limitations, it is crucial to understand\\n1https://github.com/rachtibat/LRP-for-Transformers\\nexplanation for “ dog”inputfaithfulness\\nlatent \\nattributionscomputational \\nefficiencyA ttnLRP (ours)SmoothGr adGr ad×A ttnRollA tMan\\nFigure 1. By optimizing LRP for transformer-based architectures,\\nour LRP variant outperforms other state-of-the-art methods in\\nterms of explanation faithfulness and computational efficiency.\\nWe further are able to explain latent neurons inside and outside\\nthe attention module, allowing us to interact with the model. A\\nmore detailed discussion on the differences between AttnLRP and\\nother LRP variants can be found in Appendix A.2.2. Heatmaps\\nfor other methods are illustrated in Appendix Figure 6. Legend:\\nhighly ( +), semi- ( ◦), not suited ( −). Credit: Nataba/iStock.\\nthe latent reasoning process of transformer models. Re-\\nsearchers started using the attention mechanism of trans-\\nformers as a means to understand how input tokens inter-\\nact with each other. Attention maps contain rich informa-\\ntion about the data distribution (Clark et al., 2019; Caron\\net al., 2021), even allowing for image data segmentation.\\nHowever, attention, by itself, is inadequate for compre-\\nhending the full spectrum of model behavior (Wiegreffe\\nand Pinter, 2019). Similar to latent activations, attention\\nis not class-specific and solely provides an explanation for\\nthe softmax output (in attention layers) while disregarding\\nother model components. Recent works (Geva et al., 2021;\\nDai et al., 2022), e.g., have discovered that factual knowl-\\nedge in Large Language Models (LLMs) is stored in Feed-\\nForward Network (FFN) neurons, separate from attention\\nlayers. Further, attention-based attribution methods such\\nas rollout (Abnar and Zuidema, 2020; Chefer et al., 2021a)\\n1arXiv:2402.05602v1  [cs.CL]  8 Feb 2024',\n",
       " 'start_char_idx': None,\n",
       " 'end_char_idx': None,\n",
       " 'text_template': '{metadata_str}\\n\\n{content}',\n",
       " 'metadata_template': '{key}: {value}',\n",
       " 'metadata_seperator': '\\n'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "Look at the file contents\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Summarize the nature and content of the document for organization\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models, also known as efficient or sparse language models, are a type of neural network architecture designed to process and generate human-like text at a much faster pace than traditional recurrent neural networks (RNNs). The importance of fast language models lies in their ability to improve the efficiency of natural language processing (NLP) tasks, making them a crucial component in many applications.\n",
      "\n",
      "Here are some key reasons why fast language models are important:\n",
      "\n",
      "1. **Fast processing and latency reduction**: Fast language models are designed to process text sequences quickly, which is essential for applications that require real-time text processing, such as chatbots, virtual assistants, and language translation systems. By reducing processing times, these models enable faster response times and better user experiences.\n",
      "2. **Scalability and parallelization**: Fast language models can be easily parallelized and distributed across multiple CPUs or GPUs, making them suitable for large-scale NLP applications that require processing vast amounts of text data. This scalability enables the processing of big data and improves the efficiency of NLP workflows.\n",
      "3. **Improved performance in low-resource scenarios**: Fast language models can operate effectively even in low-resource situations, such as when computing resources are limited or when there is a lack of labeled training data. This makes them suitable for deployment in resource-constrained environments, such as mobile devices or embedded systems.\n",
      "4. **Enhanced performance in NLP tasks**: Fast language models have been shown to improve the performance of various NLP tasks, such as:\n",
      "\t* **Text classification**: By reducing the computation required for text classification, fast language models can lead to better classification accuracy and faster processing times.\n",
      "\t* **Language translation**: Fast language models can enable faster and more accurate translation, which is critical for applications like machine translation and subtitling.\n",
      "\t* **Summarization and question answering**: By efficiently processing large documents, fast language models can help generate accurate summaries and answer questions more quickly.\n",
      "5. **Advancements in AI and NLP research**: Fast language models have contributed to significant advancements in AI and NLP research, driving innovation in areas like:\n",
      "\t* **Natural language processing**: Fast language models have accelerated progress in NLP, enabling researchers to explore new applications and improve existing ones.\n",
      "\t* **Artificial intelligence**: The development of fast language models has pushed the boundaries of AI, demonstrating the potential for efficient and scalable AI-powered applications.\n",
      "6. **Practical applications**: Fast language models are already being used in various industries, such as:\n",
      "\t* **Customer service**: Fast language models can be used to power chatbots and virtual assistants, improving customer service and response times.\n",
      "\t* **Content creation**: Fast language models can help generate written content quickly, such as news articles or social media posts.\n",
      "\t* **Marketing and advertising**: Fast language models can be used to analyze large amounts of text data, improving marketing and advertising strategies.\n",
      "\n",
      "In summary, fast language models are essential for improving the efficiency and scalability of NLP tasks, enabling applications that require fast text processing, and driving advancements in AI research. Their impact has far-reaching implications for various industries and applications, making them a crucial component in the development of modern AI systems.\n"
     ]
    }
   ],
   "source": [
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2402.05602v1.pdf', 'd20e3b9a-9981-41c4-a6f7-dc1bafff4761.JPG']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='8c36b329-a118-48e2-8e89-88d52aa8700f', embedding=None, metadata={'page_label': '1', 'file_name': '2402.05602v1.pdf', 'file_path': '/Users/ashwin/side_projects/llama-fs/notebooks/../sample_data/2402.05602v1.pdf', 'file_type': 'application/pdf', 'file_size': 14982917, 'creation_date': '2024-05-11', 'last_modified_date': '2024-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='AttnLRP: Attention-Aware Layer-wise Relevance Propagation\\nfor Transformers\\nReduan Achtibat1Sayed Mohammad Vakilzadeh Hatefi1Maximilian Dreyer1\\nAakriti Jain1Thomas Wiegand1,2,3Sebastian Lapuschkin1,†Wojciech Samek1,2,3,†\\n1Fraunhofer Heinrich-Hertz-Institute, 10587 Berlin, Germany\\n2Technische Universit ¨at Berlin, 10587 Berlin, Germany\\n3BIFOLD – Berlin Institute for the Foundations of Learning and Data, 10587 Berlin, Germany\\n†corresponding authors: {wojciech.samek,sebastian.lapuschkin }@hhi.fraunhofer.de\\nAbstract\\nLarge Language Models are prone to biased\\npredictions and hallucinations, underlining the\\nparamount importance of understanding their\\nmodel-internal reasoning process. However,\\nachieving faithful attributions for the entirety\\nof a black-box transformer model and main-\\ntaining computational efficiency is an unsolved\\nchallenge. By extending the Layer-wise Rel-\\nevance Propagation attribution method to han-\\ndle attention layers, we address these challenges\\neffectively. While partial solutions exist, our\\nmethod is the first to faithfully and holistically\\nattribute not only input but also latent represen-\\ntations of transformer models with the computa-\\ntional efficiency similar to a singular backward\\npass. Through extensive evaluations against ex-\\nisting methods on Llama 2, Flan-T5 and the Vi-\\nsion Transformer architecture, we demonstrate\\nthat our proposed approach surpasses alterna-\\ntive methods in terms of faithfulness and enables\\nthe understanding of latent representations, open-\\ning up the door for concept-based explanations.\\nWe provide an open-source implementation on\\nGitHub1.\\n1. Introduction\\nThe attention mechanism (Vaswani et al., 2017) became\\nan essential component of large transformers due to its\\nunique ability to handle multimodality and to scale to bil-\\nlions of training samples. While these models demonstrate\\nimpressive performance in text and image generation, they\\nare prone to biased predictions and hallucinations (Huang\\net al., 2023), which hamper their widespread adoption.\\nTo overcome these limitations, it is crucial to understand\\n1https://github.com/rachtibat/LRP-for-Transformers\\nexplanation for “ dog”inputfaithfulness\\nlatent \\nattributionscomputational \\nefficiencyA ttnLRP (ours)SmoothGr adGr ad×A ttnRollA tMan\\nFigure 1. By optimizing LRP for transformer-based architectures,\\nour LRP variant outperforms other state-of-the-art methods in\\nterms of explanation faithfulness and computational efficiency.\\nWe further are able to explain latent neurons inside and outside\\nthe attention module, allowing us to interact with the model. A\\nmore detailed discussion on the differences between AttnLRP and\\nother LRP variants can be found in Appendix A.2.2. Heatmaps\\nfor other methods are illustrated in Appendix Figure 6. Legend:\\nhighly ( +), semi- ( ◦), not suited ( −). Credit: Nataba/iStock.\\nthe latent reasoning process of transformer models. Re-\\nsearchers started using the attention mechanism of trans-\\nformers as a means to understand how input tokens inter-\\nact with each other. Attention maps contain rich informa-\\ntion about the data distribution (Clark et al., 2019; Caron\\net al., 2021), even allowing for image data segmentation.\\nHowever, attention, by itself, is inadequate for compre-\\nhending the full spectrum of model behavior (Wiegreffe\\nand Pinter, 2019). Similar to latent activations, attention\\nis not class-specific and solely provides an explanation for\\nthe softmax output (in attention layers) while disregarding\\nother model components. Recent works (Geva et al., 2021;\\nDai et al., 2022), e.g., have discovered that factual knowl-\\nedge in Large Language Models (LLMs) is stored in Feed-\\nForward Network (FFN) neurons, separate from attention\\nlayers. Further, attention-based attribution methods such\\nas rollout (Abnar and Zuidema, 2020; Chefer et al., 2021a)\\n1arXiv:2402.05602v1  [cs.CL]  8 Feb 2024', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.pull('llama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embedding': [-2.577517032623291,\n",
       "  1.5360677242279053,\n",
       "  -4.998623847961426,\n",
       "  0.280786395072937,\n",
       "  1.0266965627670288,\n",
       "  1.0111560821533203,\n",
       "  -4.2041335105896,\n",
       "  -0.540476381778717,\n",
       "  -2.1712558269500732,\n",
       "  -0.16592812538146973,\n",
       "  0.7933992743492126,\n",
       "  -1.4778883457183838,\n",
       "  2.5061886310577393,\n",
       "  0.2922838032245636,\n",
       "  3.5912506580352783,\n",
       "  -1.7778886556625366,\n",
       "  0.8803485035896301,\n",
       "  -0.2269447147846222,\n",
       "  0.010018043220043182,\n",
       "  -0.4682029187679291,\n",
       "  -2.478519916534424,\n",
       "  -1.6167516708374023,\n",
       "  1.533035397529602,\n",
       "  1.5329570770263672,\n",
       "  -1.2227483987808228,\n",
       "  2.208388566970825,\n",
       "  -0.6601111888885498,\n",
       "  0.673115611076355,\n",
       "  4.746397018432617,\n",
       "  -2.4699203968048096,\n",
       "  0.34411707520484924,\n",
       "  3.217918634414673,\n",
       "  -1.2842612266540527,\n",
       "  -1.9234678745269775,\n",
       "  5.781516075134277,\n",
       "  -0.9594265222549438,\n",
       "  -2.2439255714416504,\n",
       "  -4.515259265899658,\n",
       "  -1.070086121559143,\n",
       "  0.779033899307251,\n",
       "  0.66457200050354,\n",
       "  0.0985439121723175,\n",
       "  0.24888473749160767,\n",
       "  -0.07578709721565247,\n",
       "  1.5423295497894287,\n",
       "  1.050765872001648,\n",
       "  0.5499398708343506,\n",
       "  3.1391634941101074,\n",
       "  0.16105236113071442,\n",
       "  -0.3093404471874237,\n",
       "  -0.701096773147583,\n",
       "  0.24790047109127045,\n",
       "  1.4383912086486816,\n",
       "  -0.6984767913818359,\n",
       "  -2.29353928565979,\n",
       "  0.057276688516139984,\n",
       "  -0.5354859232902527,\n",
       "  -0.43005281686782837,\n",
       "  0.030092013999819756,\n",
       "  -0.3446720242500305,\n",
       "  -3.6946330070495605,\n",
       "  -2.648690938949585,\n",
       "  -1.5301752090454102,\n",
       "  1.6108715534210205,\n",
       "  1.2608952522277832,\n",
       "  3.2635772228240967,\n",
       "  0.7528098225593567,\n",
       "  -0.051611367613077164,\n",
       "  -1.275058627128601,\n",
       "  0.6951087117195129,\n",
       "  3.0121681690216064,\n",
       "  -2.3637492656707764,\n",
       "  1.4966403245925903,\n",
       "  3.318476915359497,\n",
       "  0.6965689659118652,\n",
       "  0.14942365884780884,\n",
       "  -1.821516990661621,\n",
       "  0.7904956936836243,\n",
       "  0.8165128827095032,\n",
       "  0.9177249670028687,\n",
       "  -5.885420322418213,\n",
       "  0.1578398048877716,\n",
       "  -1.0049561262130737,\n",
       "  0.07105477154254913,\n",
       "  -0.9068965315818787,\n",
       "  1.668605923652649,\n",
       "  2.995836019515991,\n",
       "  0.9675924777984619,\n",
       "  2.7218973636627197,\n",
       "  -0.0731734037399292,\n",
       "  4.451124668121338,\n",
       "  2.9658684730529785,\n",
       "  -0.7637804746627808,\n",
       "  -2.997608184814453,\n",
       "  0.3558332920074463,\n",
       "  2.269920587539673,\n",
       "  -0.6717167496681213,\n",
       "  3.3109092712402344,\n",
       "  1.1432762145996094,\n",
       "  1.1260672807693481,\n",
       "  3.014162063598633,\n",
       "  -1.1172475814819336,\n",
       "  -3.2571699619293213,\n",
       "  -0.4705071449279785,\n",
       "  0.9384560585021973,\n",
       "  -1.0865254402160645,\n",
       "  1.3495162725448608,\n",
       "  1.433117389678955,\n",
       "  -1.9792779684066772,\n",
       "  -0.673891007900238,\n",
       "  3.027097225189209,\n",
       "  3.9027671813964844,\n",
       "  -0.8002236485481262,\n",
       "  -3.9336280822753906,\n",
       "  0.16272586584091187,\n",
       "  -1.0068800449371338,\n",
       "  2.6173107624053955,\n",
       "  -1.4140480756759644,\n",
       "  -1.3966296911239624,\n",
       "  0.4775806665420532,\n",
       "  -1.9247088432312012,\n",
       "  2.2004005908966064,\n",
       "  3.536120891571045,\n",
       "  -1.3456960916519165,\n",
       "  3.6211063861846924,\n",
       "  -0.6658744812011719,\n",
       "  -1.6226977109909058,\n",
       "  -0.024757593870162964,\n",
       "  5.012316703796387,\n",
       "  3.134406089782715,\n",
       "  -1.7100073099136353,\n",
       "  -1.9037806987762451,\n",
       "  0.5543814897537231,\n",
       "  4.247457504272461,\n",
       "  0.5288041234016418,\n",
       "  -1.9774848222732544,\n",
       "  1.1648049354553223,\n",
       "  -2.2507967948913574,\n",
       "  -2.931039333343506,\n",
       "  -0.2954252362251282,\n",
       "  -2.3414316177368164,\n",
       "  -0.7217730283737183,\n",
       "  -3.959348201751709,\n",
       "  -1.49863600730896,\n",
       "  -3.0843429565429688,\n",
       "  0.9673855304718018,\n",
       "  2.1925902366638184,\n",
       "  0.3409010171890259,\n",
       "  1.9945974349975586,\n",
       "  -3.4904892444610596,\n",
       "  -3.3492588996887207,\n",
       "  -0.5377612709999084,\n",
       "  3.3260278701782227,\n",
       "  -2.9283764362335205,\n",
       "  2.250354766845703,\n",
       "  0.9519906044006348,\n",
       "  -2.8828182220458984,\n",
       "  -2.979720115661621,\n",
       "  2.187278985977173,\n",
       "  -0.4857114851474762,\n",
       "  -3.3203413486480713,\n",
       "  -0.21468311548233032,\n",
       "  -3.0182344913482666,\n",
       "  -0.20962749421596527,\n",
       "  -6.133942127227783,\n",
       "  2.523355007171631,\n",
       "  5.199891090393066,\n",
       "  4.263858795166016,\n",
       "  -0.5157493948936462,\n",
       "  -3.0300073623657227,\n",
       "  3.0896003246307373,\n",
       "  -1.5256136655807495,\n",
       "  -0.3903810381889343,\n",
       "  -1.2605242729187012,\n",
       "  -0.9774015545845032,\n",
       "  -3.9715993404388428,\n",
       "  0.1013699620962143,\n",
       "  -3.3806581497192383,\n",
       "  -0.3892595171928406,\n",
       "  0.10282667726278305,\n",
       "  1.6892931461334229,\n",
       "  -3.68664288520813,\n",
       "  -0.018143199384212494,\n",
       "  -2.5650036334991455,\n",
       "  6.164673328399658,\n",
       "  0.4677096903324127,\n",
       "  4.894288063049316,\n",
       "  2.0284695625305176,\n",
       "  -0.4475424587726593,\n",
       "  1.603348970413208,\n",
       "  0.04871534928679466,\n",
       "  0.40079158544540405,\n",
       "  0.6149754524230957,\n",
       "  -1.1530139446258545,\n",
       "  2.9509048461914062,\n",
       "  0.7856464982032776,\n",
       "  0.46122390031814575,\n",
       "  0.032319389283657074,\n",
       "  -1.0826727151870728,\n",
       "  -0.3096473813056946,\n",
       "  -0.6516581773757935,\n",
       "  0.1568668931722641,\n",
       "  1.6334524154663086,\n",
       "  0.3016200363636017,\n",
       "  3.9927332401275635,\n",
       "  0.5607341527938843,\n",
       "  -1.0049538612365723,\n",
       "  3.9518277645111084,\n",
       "  2.1906747817993164,\n",
       "  0.7068613767623901,\n",
       "  -1.1996959447860718,\n",
       "  -0.24058221280574799,\n",
       "  -3.3431012630462646,\n",
       "  2.224691867828369,\n",
       "  -3.6453731060028076,\n",
       "  -0.35404953360557556,\n",
       "  0.4467448592185974,\n",
       "  1.142985224723816,\n",
       "  3.102039098739624,\n",
       "  1.3155179023742676,\n",
       "  4.407995700836182,\n",
       "  2.628005027770996,\n",
       "  5.234172344207764,\n",
       "  -2.6177613735198975,\n",
       "  1.0955790281295776,\n",
       "  -1.2603437900543213,\n",
       "  0.8627581596374512,\n",
       "  0.23106688261032104,\n",
       "  2.673072338104248,\n",
       "  -0.4200223386287689,\n",
       "  2.026320219039917,\n",
       "  -2.303863525390625,\n",
       "  -1.6021296977996826,\n",
       "  -3.035903215408325,\n",
       "  1.2413626909255981,\n",
       "  2.138597011566162,\n",
       "  5.181971073150635,\n",
       "  -0.4185422956943512,\n",
       "  -2.918422222137451,\n",
       "  1.6820144653320312,\n",
       "  -1.4159808158874512,\n",
       "  2.844011068344116,\n",
       "  1.9481098651885986,\n",
       "  4.547475337982178,\n",
       "  0.1426510214805603,\n",
       "  -0.07659020274877548,\n",
       "  2.270170211791992,\n",
       "  -0.7096051573753357,\n",
       "  -2.2331278324127197,\n",
       "  1.9346874952316284,\n",
       "  -2.341503620147705,\n",
       "  0.5820223093032837,\n",
       "  2.824626922607422,\n",
       "  -0.07056191563606262,\n",
       "  0.5786635875701904,\n",
       "  -0.8968363404273987,\n",
       "  1.104645013809204,\n",
       "  1.82863187789917,\n",
       "  -0.47024813294410706,\n",
       "  0.8367140293121338,\n",
       "  1.2415598630905151,\n",
       "  -0.17951659858226776,\n",
       "  -0.448904812335968,\n",
       "  1.7475354671478271,\n",
       "  -1.8035954236984253,\n",
       "  2.1851601600646973,\n",
       "  -0.07479412853717804,\n",
       "  1.264899492263794,\n",
       "  -3.5165281295776367,\n",
       "  2.6974010467529297,\n",
       "  -0.05020415410399437,\n",
       "  0.41926679015159607,\n",
       "  -2.4392051696777344,\n",
       "  1.8062918186187744,\n",
       "  -1.916502833366394,\n",
       "  -2.3022351264953613,\n",
       "  2.169753074645996,\n",
       "  1.7467044591903687,\n",
       "  -0.3812267482280731,\n",
       "  0.3938131034374237,\n",
       "  0.36319226026535034,\n",
       "  -14.242216110229492,\n",
       "  -1.3819471597671509,\n",
       "  0.5261126756668091,\n",
       "  -2.06514048576355,\n",
       "  1.3975353240966797,\n",
       "  1.2140862941741943,\n",
       "  0.8072736859321594,\n",
       "  4.868049144744873,\n",
       "  -6.814724922180176,\n",
       "  -0.6291948556900024,\n",
       "  -1.6144139766693115,\n",
       "  -5.101278781890869,\n",
       "  -4.010212421417236,\n",
       "  1.3218704462051392,\n",
       "  0.6502326130867004,\n",
       "  2.183845281600952,\n",
       "  -2.252974510192871,\n",
       "  -1.6766259670257568,\n",
       "  -3.6261470317840576,\n",
       "  1.0445339679718018,\n",
       "  -1.4677324295043945,\n",
       "  0.8735345602035522,\n",
       "  -1.6357333660125732,\n",
       "  0.45393067598342896,\n",
       "  -1.3244882822036743,\n",
       "  1.5510915517807007,\n",
       "  0.7680498361587524,\n",
       "  -3.173374891281128,\n",
       "  3.552494525909424,\n",
       "  -0.7195969223976135,\n",
       "  -2.8346333503723145,\n",
       "  -2.015761137008667,\n",
       "  -1.1880728006362915,\n",
       "  2.9440455436706543,\n",
       "  -4.083583831787109,\n",
       "  -2.5216991901397705,\n",
       "  -2.704317808151245,\n",
       "  -0.19943620264530182,\n",
       "  -0.08749198913574219,\n",
       "  -1.9635471105575562,\n",
       "  0.45113956928253174,\n",
       "  0.913942277431488,\n",
       "  -0.6294040679931641,\n",
       "  -3.3542912006378174,\n",
       "  -1.9050911664962769,\n",
       "  -2.189593553543091,\n",
       "  -0.4695487916469574,\n",
       "  -0.21375282108783722,\n",
       "  -3.9625492095947266,\n",
       "  -1.1778247356414795,\n",
       "  0.7973718643188477,\n",
       "  -1.5398887395858765,\n",
       "  3.8147194385528564,\n",
       "  0.49615949392318726,\n",
       "  0.537476122379303,\n",
       "  4.528961181640625,\n",
       "  -0.6343704462051392,\n",
       "  0.366296648979187,\n",
       "  -0.4194062650203705,\n",
       "  -1.328686237335205,\n",
       "  -0.28862258791923523,\n",
       "  -1.315243124961853,\n",
       "  -2.7092318534851074,\n",
       "  -1.3832460641860962,\n",
       "  0.007820473983883858,\n",
       "  0.4006894528865814,\n",
       "  -0.9804700613021851,\n",
       "  0.7758448123931885,\n",
       "  2.7118477821350098,\n",
       "  -0.2829265594482422,\n",
       "  1.9961357116699219,\n",
       "  -2.0164053440093994,\n",
       "  1.3146681785583496,\n",
       "  -3.23175048828125,\n",
       "  0.04567403346300125,\n",
       "  -0.040237754583358765,\n",
       "  4.448444843292236,\n",
       "  0.038733694702386856,\n",
       "  2.5028464794158936,\n",
       "  0.7049996852874756,\n",
       "  0.6560541987419128,\n",
       "  -0.7990111708641052,\n",
       "  -0.6989339590072632,\n",
       "  -0.9597935080528259,\n",
       "  -3.196838855743408,\n",
       "  -1.4671766757965088,\n",
       "  4.487083435058594,\n",
       "  1.098122000694275,\n",
       "  -1.1419461965560913,\n",
       "  -0.8632090091705322,\n",
       "  0.2640428841114044,\n",
       "  2.638417959213257,\n",
       "  -0.07397104054689407,\n",
       "  1.1495193243026733,\n",
       "  -0.04517103359103203,\n",
       "  4.178229808807373,\n",
       "  1.92386794090271,\n",
       "  -2.188345432281494,\n",
       "  1.364113450050354,\n",
       "  1.8650072813034058,\n",
       "  -2.5497779846191406,\n",
       "  -2.4311375617980957,\n",
       "  1.646966814994812,\n",
       "  -1.5577689409255981,\n",
       "  -3.7820639610290527,\n",
       "  1.0763647556304932,\n",
       "  -0.706070601940155,\n",
       "  0.6450326442718506,\n",
       "  1.0579991340637207,\n",
       "  -1.7313820123672485,\n",
       "  2.2775087356567383,\n",
       "  2.832564353942871,\n",
       "  4.990604877471924,\n",
       "  0.9511741995811462,\n",
       "  0.9374122619628906,\n",
       "  0.6660997271537781,\n",
       "  4.379657745361328,\n",
       "  -3.821512460708618,\n",
       "  -0.5733705163002014,\n",
       "  0.5329802632331848,\n",
       "  2.0479836463928223,\n",
       "  1.2886382341384888,\n",
       "  -4.638825416564941,\n",
       "  1.558824062347412,\n",
       "  0.8076350092887878,\n",
       "  -3.502655267715454,\n",
       "  -1.3341950178146362,\n",
       "  -2.3701324462890625,\n",
       "  -0.6595522165298462,\n",
       "  -0.18567174673080444,\n",
       "  -5.0102996826171875,\n",
       "  -0.2577192783355713,\n",
       "  2.2600715160369873,\n",
       "  1.0372813940048218,\n",
       "  -0.052730850875377655,\n",
       "  0.8185799717903137,\n",
       "  0.5170404314994812,\n",
       "  -3.800226926803589,\n",
       "  -2.39254093170166,\n",
       "  -1.567755103111267,\n",
       "  0.209071084856987,\n",
       "  -1.004381537437439,\n",
       "  4.6278886795043945,\n",
       "  -1.5161312818527222,\n",
       "  0.12245864421129227,\n",
       "  -2.7955379486083984,\n",
       "  1.8487083911895752,\n",
       "  -0.48013177514076233,\n",
       "  1.035674810409546,\n",
       "  -3.45182204246521,\n",
       "  2.057173013687134,\n",
       "  4.204331874847412,\n",
       "  -3.346046209335327,\n",
       "  -4.364371299743652,\n",
       "  1.994740605354309,\n",
       "  -5.522571086883545,\n",
       "  1.5497318506240845,\n",
       "  -1.1730372905731201,\n",
       "  2.773984909057617,\n",
       "  0.852608323097229,\n",
       "  -1.0834330320358276,\n",
       "  -0.9804481267929077,\n",
       "  -2.295790672302246,\n",
       "  -3.1399312019348145,\n",
       "  0.5497123003005981,\n",
       "  -1.9302022457122803,\n",
       "  1.340286374092102,\n",
       "  -1.9627277851104736,\n",
       "  1.1450858116149902,\n",
       "  -0.6197598576545715,\n",
       "  2.790670156478882,\n",
       "  -2.31392765045166,\n",
       "  2.4894192218780518,\n",
       "  -0.7937084436416626,\n",
       "  0.3652668297290802,\n",
       "  -3.0510029792785645,\n",
       "  0.5784081816673279,\n",
       "  3.8041136264801025,\n",
       "  1.2087936401367188,\n",
       "  2.8101019859313965,\n",
       "  2.247685194015503,\n",
       "  -3.410532236099243,\n",
       "  0.6288610696792603,\n",
       "  -1.2534208297729492,\n",
       "  -0.5973612666130066,\n",
       "  -0.3852366805076599,\n",
       "  -2.447626829147339,\n",
       "  2.48954701423645,\n",
       "  0.4591197967529297,\n",
       "  -3.201488494873047,\n",
       "  0.15320761501789093,\n",
       "  -1.0863124132156372,\n",
       "  0.861497163772583,\n",
       "  -1.5591983795166016,\n",
       "  0.4085189402103424,\n",
       "  -2.1501872539520264,\n",
       "  -1.0986932516098022,\n",
       "  -1.9258723258972168,\n",
       "  -1.3436065912246704,\n",
       "  -0.525529146194458,\n",
       "  -0.5616166591644287,\n",
       "  -0.4706459939479828,\n",
       "  1.015882134437561,\n",
       "  3.2813916206359863,\n",
       "  2.6664364337921143,\n",
       "  -3.6019539833068848,\n",
       "  -0.8154260516166687,\n",
       "  -1.376228928565979,\n",
       "  1.4123761653900146,\n",
       "  2.4259793758392334,\n",
       "  -3.3631181716918945,\n",
       "  -1.0594391822814941,\n",
       "  -4.226432800292969,\n",
       "  0.8015809655189514,\n",
       "  -1.3772754669189453,\n",
       "  2.6995162963867188,\n",
       "  0.5372109413146973,\n",
       "  -1.4866997003555298,\n",
       "  0.9886007308959961,\n",
       "  0.10556290298700333,\n",
       "  0.7356910705566406,\n",
       "  2.3678090572357178,\n",
       "  2.879446029663086,\n",
       "  -6.051049709320068,\n",
       "  -0.9472969770431519,\n",
       "  -2.2430968284606934,\n",
       "  1.1703858375549316,\n",
       "  -0.665501594543457,\n",
       "  0.17546164989471436,\n",
       "  -0.13898169994354248,\n",
       "  -3.8345212936401367,\n",
       "  -1.341442584991455,\n",
       "  3.469581127166748,\n",
       "  0.7626229524612427,\n",
       "  -2.6604878902435303,\n",
       "  -1.497853398323059,\n",
       "  0.8077916502952576,\n",
       "  -2.7949795722961426,\n",
       "  1.1333816051483154,\n",
       "  -2.1787447929382324,\n",
       "  -1.477653980255127,\n",
       "  -4.408339023590088,\n",
       "  1.9698553085327148,\n",
       "  1.4017601013183594,\n",
       "  1.6509854793548584,\n",
       "  2.8736605644226074,\n",
       "  3.396089553833008,\n",
       "  -2.574352502822876,\n",
       "  2.2390408515930176,\n",
       "  -1.7864933013916016,\n",
       "  -1.360569953918457,\n",
       "  2.0570623874664307,\n",
       "  1.584525465965271,\n",
       "  -1.3260570764541626,\n",
       "  -0.20304527878761292,\n",
       "  4.904629707336426,\n",
       "  -0.4985002875328064,\n",
       "  -4.579863548278809,\n",
       "  1.9533814191818237,\n",
       "  -1.1757218837738037,\n",
       "  -0.25348687171936035,\n",
       "  -1.6776930093765259,\n",
       "  -0.845106303691864,\n",
       "  -1.3573436737060547,\n",
       "  -0.7180700302124023,\n",
       "  -0.7691591382026672,\n",
       "  2.1729469299316406,\n",
       "  -1.3766683340072632,\n",
       "  -0.986770749092102,\n",
       "  -4.356163501739502,\n",
       "  0.2072683572769165,\n",
       "  4.093662738800049,\n",
       "  -0.22206515073776245,\n",
       "  2.5243892669677734,\n",
       "  -0.8715286254882812,\n",
       "  3.0259809494018555,\n",
       "  -0.5781732797622681,\n",
       "  2.274474620819092,\n",
       "  -0.400328129529953,\n",
       "  -2.37967586517334,\n",
       "  1.0423407554626465,\n",
       "  -0.5062991380691528,\n",
       "  1.0294489860534668,\n",
       "  1.8932639360427856,\n",
       "  1.4873391389846802,\n",
       "  2.0996720790863037,\n",
       "  2.0362257957458496,\n",
       "  1.021335482597351,\n",
       "  -5.825254917144775,\n",
       "  2.1636221408843994,\n",
       "  0.3415197432041168,\n",
       "  -1.7167266607284546,\n",
       "  3.543597936630249,\n",
       "  -1.237117886543274,\n",
       "  -0.3002799451351166,\n",
       "  -2.3426780700683594,\n",
       "  3.158963203430176,\n",
       "  2.1141879558563232,\n",
       "  -2.8939249515533447,\n",
       "  -4.140758037567139,\n",
       "  -0.2526198625564575,\n",
       "  -0.3931885063648224,\n",
       "  1.0989001989364624,\n",
       "  -1.0826663970947266,\n",
       "  -2.1930410861968994,\n",
       "  -0.16973823308944702,\n",
       "  0.6634597778320312,\n",
       "  -3.628641128540039,\n",
       "  -1.7164242267608643,\n",
       "  0.2859359383583069,\n",
       "  2.3143324851989746,\n",
       "  1.0499757528305054,\n",
       "  -6.623737812042236,\n",
       "  -3.4179370403289795,\n",
       "  1.5429054498672485,\n",
       "  -0.63792484998703,\n",
       "  1.5892720222473145,\n",
       "  0.07708339393138885,\n",
       "  -2.2489821910858154,\n",
       "  -1.964953899383545,\n",
       "  2.838616371154785,\n",
       "  -0.18786726891994476,\n",
       "  -0.9452924132347107,\n",
       "  0.43462350964546204,\n",
       "  0.56429123878479,\n",
       "  -3.580782651901245,\n",
       "  -1.1988248825073242,\n",
       "  2.9073972702026367,\n",
       "  1.7508183717727661,\n",
       "  2.071988344192505,\n",
       "  -0.33358675241470337,\n",
       "  0.47061172127723694,\n",
       "  -7.869272232055664,\n",
       "  -1.528244972229004,\n",
       "  0.678867518901825,\n",
       "  -0.22148321568965912,\n",
       "  3.7615342140197754,\n",
       "  2.4135546684265137,\n",
       "  0.32232850790023804,\n",
       "  2.784705877304077,\n",
       "  2.900690793991089,\n",
       "  -1.9959558248519897,\n",
       "  3.362534999847412,\n",
       "  0.8557076454162598,\n",
       "  -5.769106864929199,\n",
       "  -0.14534008502960205,\n",
       "  0.8682090640068054,\n",
       "  0.7141469120979309,\n",
       "  -2.1393179893493652,\n",
       "  -1.8325369358062744,\n",
       "  -0.38548558950424194,\n",
       "  -4.613010883331299,\n",
       "  5.249390602111816,\n",
       "  0.788856565952301,\n",
       "  -1.0473051071166992,\n",
       "  0.5832448601722717,\n",
       "  -0.2825721204280853,\n",
       "  3.3500208854675293,\n",
       "  2.1700539588928223,\n",
       "  -2.105512857437134,\n",
       "  -5.871976375579834,\n",
       "  1.8717012405395508,\n",
       "  0.228361114859581,\n",
       "  -1.5917868614196777,\n",
       "  -0.9370385408401489,\n",
       "  -2.2414402961730957,\n",
       "  -3.721014976501465,\n",
       "  2.1704652309417725,\n",
       "  0.6267156600952148,\n",
       "  -0.1604526937007904,\n",
       "  5.448962211608887,\n",
       "  -3.2246365547180176,\n",
       "  -2.9865872859954834,\n",
       "  -2.222184896469116,\n",
       "  -0.5760416388511658,\n",
       "  -1.2088887691497803,\n",
       "  -1.0632059574127197,\n",
       "  -1.726899266242981,\n",
       "  -0.19125235080718994,\n",
       "  -0.38575565814971924,\n",
       "  2.8409290313720703,\n",
       "  5.365472316741943,\n",
       "  -2.3382139205932617,\n",
       "  -0.8456991314888,\n",
       "  -0.5304000973701477,\n",
       "  -1.2030537128448486,\n",
       "  -0.19228807091712952,\n",
       "  0.5158021450042725,\n",
       "  -1.340549349784851,\n",
       "  2.443251609802246,\n",
       "  -0.8805726766586304,\n",
       "  5.36417818069458,\n",
       "  -1.3164323568344116,\n",
       "  3.0588982105255127,\n",
       "  0.47164037823677063,\n",
       "  0.0640401840209961,\n",
       "  2.33343505859375,\n",
       "  2.814695358276367,\n",
       "  -1.9140574932098389,\n",
       "  -2.3303565979003906,\n",
       "  3.9281792640686035,\n",
       "  1.4642338752746582,\n",
       "  0.7747719883918762,\n",
       "  -0.010811909101903439,\n",
       "  -0.4146232008934021,\n",
       "  1.8005636930465698,\n",
       "  1.8130980730056763,\n",
       "  0.6078423261642456,\n",
       "  -2.575977087020874,\n",
       "  -0.6812153458595276,\n",
       "  1.1652227640151978,\n",
       "  4.64026403427124,\n",
       "  -0.10292112827301025,\n",
       "  -0.7833565473556519,\n",
       "  -0.06242002174258232,\n",
       "  4.024423122406006,\n",
       "  -0.7988998889923096,\n",
       "  -0.9053333401679993,\n",
       "  -0.26426705718040466,\n",
       "  2.581913471221924,\n",
       "  3.3964788913726807,\n",
       "  2.858004093170166,\n",
       "  1.0506319999694824,\n",
       "  1.895951509475708,\n",
       "  -0.9932504892349243,\n",
       "  -0.9338337182998657,\n",
       "  -2.613077163696289,\n",
       "  -2.1340560913085938,\n",
       "  2.376594305038452,\n",
       "  -0.290327250957489,\n",
       "  -4.344979286193848,\n",
       "  -2.66044020652771,\n",
       "  -2.6355836391448975,\n",
       "  -0.7462750673294067,\n",
       "  0.5723612308502197,\n",
       "  -1.5811713933944702,\n",
       "  -0.05728111043572426,\n",
       "  -0.04948078468441963,\n",
       "  -1.940228819847107,\n",
       "  1.3934372663497925,\n",
       "  0.7833378314971924,\n",
       "  1.9048460721969604,\n",
       "  2.678473711013794,\n",
       "  -1.183504581451416,\n",
       "  -2.1306753158569336,\n",
       "  0.5956253409385681,\n",
       "  0.2557823359966278,\n",
       "  -1.8133798837661743,\n",
       "  -1.3315141201019287,\n",
       "  1.5992883443832397,\n",
       "  0.6339951753616333,\n",
       "  -2.2537128925323486,\n",
       "  -1.9838218688964844,\n",
       "  0.08182910829782486,\n",
       "  -2.3405280113220215,\n",
       "  2.0055127143859863,\n",
       "  -3.866750955581665,\n",
       "  -4.216854572296143,\n",
       "  2.850475549697876,\n",
       "  2.0173840522766113,\n",
       "  0.47720667719841003,\n",
       "  -5.232626438140869,\n",
       "  1.1648285388946533,\n",
       "  0.4131595492362976,\n",
       "  -0.5521210432052612,\n",
       "  -1.2668980360031128,\n",
       "  1.387726068496704,\n",
       "  1.801445722579956,\n",
       "  -2.2218010425567627,\n",
       "  0.7681233286857605,\n",
       "  -3.5188136100769043,\n",
       "  1.0354161262512207,\n",
       "  0.6069526076316833,\n",
       "  5.045158386230469,\n",
       "  -2.0804619789123535,\n",
       "  4.0856547355651855,\n",
       "  0.6485365629196167,\n",
       "  -1.7978477478027344,\n",
       "  -2.8141443729400635,\n",
       "  -2.118860960006714,\n",
       "  1.537815809249878,\n",
       "  2.460514545440674,\n",
       "  2.7278687953948975,\n",
       "  -0.3972623646259308,\n",
       "  0.3783382475376129,\n",
       "  -1.3992959260940552,\n",
       "  -0.6422317028045654,\n",
       "  -0.9888588786125183,\n",
       "  1.6701661348342896,\n",
       "  -2.0472545623779297,\n",
       "  -0.32819247245788574,\n",
       "  -2.1546530723571777,\n",
       "  1.0260051488876343,\n",
       "  2.51655650138855,\n",
       "  -2.213397979736328,\n",
       "  0.9856374263763428,\n",
       "  0.4165148437023163,\n",
       "  0.7709389328956604,\n",
       "  -0.010253319516777992,\n",
       "  2.024980306625366,\n",
       "  1.5147958993911743,\n",
       "  -12.551631927490234,\n",
       "  -0.7046710252761841,\n",
       "  -1.2102446556091309,\n",
       "  -1.1145228147506714,\n",
       "  1.3881809711456299,\n",
       "  2.393340826034546,\n",
       "  -0.7155675888061523,\n",
       "  -4.29198694229126,\n",
       "  -1.1268067359924316,\n",
       "  0.03903244808316231,\n",
       "  -1.0272557735443115,\n",
       "  -0.49832549691200256,\n",
       "  -0.5079512596130371,\n",
       "  -2.3246331214904785,\n",
       "  -1.1578632593154907,\n",
       "  4.821588516235352,\n",
       "  0.15267416834831238,\n",
       "  1.7625051736831665,\n",
       "  -1.8567272424697876,\n",
       "  1.7242435216903687,\n",
       "  2.5739288330078125,\n",
       "  -2.0379927158355713,\n",
       "  -2.1560842990875244,\n",
       "  2.010272741317749,\n",
       "  -0.8894121050834656,\n",
       "  3.5316593647003174,\n",
       "  1.60740327835083,\n",
       "  -2.159900665283203,\n",
       "  -3.837618112564087,\n",
       "  0.6051198244094849,\n",
       "  -2.7365851402282715,\n",
       "  -3.6800053119659424,\n",
       "  -1.1335948705673218,\n",
       "  1.8316621780395508,\n",
       "  -1.928267478942871,\n",
       "  0.805652916431427,\n",
       "  -1.6915292739868164,\n",
       "  -3.767491340637207,\n",
       "  2.0613465309143066,\n",
       "  -1.8101541996002197,\n",
       "  -0.2588132619857788,\n",
       "  1.1309504508972168,\n",
       "  1.8955551385879517,\n",
       "  0.479767769575119,\n",
       "  -0.6924266815185547,\n",
       "  -1.748388648033142,\n",
       "  1.0932375192642212,\n",
       "  -1.1754246950149536,\n",
       "  -2.0128061771392822,\n",
       "  0.5572764873504639,\n",
       "  -0.04714449122548103,\n",
       "  0.9057161211967468,\n",
       "  -5.2090253829956055,\n",
       "  5.009405136108398,\n",
       "  -1.9217605590820312,\n",
       "  1.1272447109222412,\n",
       "  -0.1791151612997055,\n",
       "  4.535516262054443,\n",
       "  0.4217335879802704,\n",
       "  -2.2529146671295166,\n",
       "  4.1869659423828125,\n",
       "  -3.7284281253814697,\n",
       "  0.5549747347831726,\n",
       "  2.4566752910614014,\n",
       "  1.3573212623596191,\n",
       "  3.03407883644104,\n",
       "  2.483572244644165,\n",
       "  -0.2123948484659195,\n",
       "  -1.5559741258621216,\n",
       "  -0.13809411227703094,\n",
       "  1.059907078742981,\n",
       "  -2.2435381412506104,\n",
       "  0.09431605786085129,\n",
       "  -1.1768449544906616,\n",
       "  -0.8629208207130432,\n",
       "  -1.2065153121948242,\n",
       "  4.360540866851807,\n",
       "  0.05788842588663101,\n",
       "  -1.1336370706558228,\n",
       "  0.33537158370018005,\n",
       "  1.4784245491027832,\n",
       "  0.7719199657440186,\n",
       "  1.0943326950073242,\n",
       "  1.4552441835403442,\n",
       "  -1.2052209377288818,\n",
       "  -3.0299603939056396,\n",
       "  -3.739983320236206,\n",
       "  1.1469128131866455,\n",
       "  1.3541195392608643,\n",
       "  -0.04652417078614235,\n",
       "  0.6033388376235962,\n",
       "  -0.10615768283605576,\n",
       "  0.28376874327659607,\n",
       "  1.6296271085739136,\n",
       "  0.7005103230476379,\n",
       "  -1.7330291271209717,\n",
       "  0.07876016944646835,\n",
       "  0.5150572657585144,\n",
       "  0.4299789071083069,\n",
       "  1.6552366018295288,\n",
       "  1.7517187595367432,\n",
       "  -2.0387842655181885,\n",
       "  -3.091510772705078,\n",
       "  -0.6545454859733582,\n",
       "  2.277498245239258,\n",
       "  0.664012610912323,\n",
       "  -0.9407438635826111,\n",
       "  -5.1701226234436035,\n",
       "  -0.13212929666042328,\n",
       "  3.877103328704834,\n",
       "  -0.31338176131248474,\n",
       "  9.041790008544922,\n",
       "  0.2594100832939148,\n",
       "  -1.3023607730865479,\n",
       "  2.932983875274658,\n",
       "  0.615540623664856,\n",
       "  2.3062586784362793,\n",
       "  0.5314438939094543,\n",
       "  -2.786710739135742,\n",
       "  3.00443172454834,\n",
       "  1.8940638303756714,\n",
       "  -1.2094082832336426,\n",
       "  4.0112996101379395,\n",
       "  -3.6596381664276123,\n",
       "  0.24091725051403046,\n",
       "  -3.276764154434204,\n",
       "  0.5176242589950562,\n",
       "  -2.0523934364318848,\n",
       "  2.2820541858673096,\n",
       "  -0.25942716002464294,\n",
       "  -5.4865031242370605,\n",
       "  2.7498254776000977,\n",
       "  0.6203665137290955,\n",
       "  1.8187814950942993,\n",
       "  -1.005249261856079,\n",
       "  -1.2694863080978394,\n",
       "  -3.5066301822662354,\n",
       "  0.07176854461431503,\n",
       "  0.13613250851631165,\n",
       "  1.5733153820037842,\n",
       "  -0.2647199034690857,\n",
       "  0.4148922264575958,\n",
       "  0.8203385472297668,\n",
       "  1.7782975435256958,\n",
       "  -0.9342747330665588,\n",
       "  3.06732177734375,\n",
       "  -1.7207056283950806,\n",
       "  1.2880979776382446,\n",
       "  3.5043630599975586,\n",
       "  0.3518327474594116,\n",
       "  -0.07897068560123444,\n",
       "  -1.0062559843063354,\n",
       "  0.2754523754119873,\n",
       "  1.937844157218933,\n",
       "  -1.667349934577942,\n",
       "  1.3312910795211792,\n",
       "  -1.804381012916565,\n",
       "  2.0431642532348633,\n",
       "  19.630638122558594,\n",
       "  1.2909061908721924,\n",
       "  3.5792765617370605,\n",
       "  1.9938139915466309,\n",
       "  -1.0921392440795898,\n",
       "  -3.4755210876464844,\n",
       "  2.605785608291626,\n",
       "  -3.1742727756500244,\n",
       "  1.533433437347412,\n",
       "  0.3212713897228241,\n",
       "  1.5425881147384644,\n",
       "  0.688005805015564,\n",
       "  0.08292697370052338,\n",
       "  2.99035382270813,\n",
       "  0.5310682058334351,\n",
       "  2.448195219039917,\n",
       "  1.354172945022583,\n",
       "  -1.225609302520752,\n",
       "  2.462815761566162,\n",
       "  -2.1614534854888916,\n",
       "  -3.1402859687805176,\n",
       "  2.091219663619995,\n",
       "  2.9060590267181396,\n",
       "  -0.4048565626144409,\n",
       "  2.0717217922210693,\n",
       "  -3.1607372760772705,\n",
       "  -4.794811725616455,\n",
       "  2.334444522857666,\n",
       "  -0.8244237303733826,\n",
       "  0.016126787289977074,\n",
       "  -0.35697928071022034,\n",
       "  3.33583664894104,\n",
       "  2.8186769485473633,\n",
       "  3.95723557472229,\n",
       "  4.190947532653809,\n",
       "  1.1718227863311768,\n",
       "  -1.7697981595993042,\n",
       "  -5.471059322357178,\n",
       "  0.9298431277275085,\n",
       "  0.22179125249385834,\n",
       "  -0.10433772206306458,\n",
       "  -2.783982515335083,\n",
       "  -0.378142774105072,\n",
       "  -0.8110437393188477,\n",
       "  -0.3201591670513153,\n",
       "  -1.7250839471817017,\n",
       "  0.6669384241104126,\n",
       "  -3.9775118827819824,\n",
       "  -0.4356706440448761,\n",
       "  -0.9194866418838501,\n",
       "  0.6292715072631836,\n",
       "  -0.5668283104896545,\n",
       "  1.4660388231277466,\n",
       "  0.5608640313148499,\n",
       "  -4.945498943328857,\n",
       "  0.16999505460262299,\n",
       "  -2.4811160564422607,\n",
       "  -2.129901170730591,\n",
       "  -2.814915180206299,\n",
       "  -2.2315475940704346,\n",
       "  ...]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.embeddings(\n",
    "    model='llama3',\n",
    "    prompt='Llamas are members of the camelid family',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3_hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
